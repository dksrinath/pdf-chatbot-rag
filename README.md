# PDF RAG Chatbot (LangChain, Gemini, Streamlit)

A production-ready Retrieval Augmented Generation (RAG) chatbot that answers questions using one or more PDF documents as knowledge sources. Built with LangChain, Google Gemini, FAISS, and Streamlit.

---

## Demo


https://github.com/user-attachments/assets/d4afa2af-2611-4952-9ad4-96f234f708e8


> Click the image above to watch a short demo of the chatbot in action.



---

## Features

- **PDF Extraction:** Upload and process multiple PDFs; text is extracted using PyPDF2.
- **Text Chunking:** Documents are split into chunks using LangChain's `RecursiveCharacterTextSplitter` (configurable size/overlap).
- **Embeddings & Vector Store:** Chunks are embedded with Sentence Transformers and stored in a FAISS vector database.
- **Semantic Search:** Top-k relevant chunks are retrieved for each query.
- **Conversational Memory:** Multi-turn chat with context retention using LangChain's `ConversationBufferMemory`.
- **LLM Integration:** Answers generated by Google Gemini via LangChain's `ChatGoogleGenerativeAI`.
- **Streamlit UI:** Clean interface for uploading PDFs, chatting, and viewing history.
- **Dynamic Configuration:** All parameters (API keys, model names, chunk size, etc.) are set via `.env` and `src/config.py`.
- **Error Handling:** Robust error management and retry logic.

---

## Setup

1. **Clone the repository:**
   ```bash
   git clone https://github.com/dksrinath/rag-chatbot.git
   cd rag-chatbot
   ```
2. **Create a virtual environment:**
   ```bash
   python -m venv venv
   venv\Scripts\activate  # On Windows
   # Or
   source venv/bin/activate  # On Mac/Linux
   ```
3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
4. **Configure environment variables:**
   - Create a `.env` file in the root folder:
     ```
     GEMINI_API_KEY=your-gemini-api-key-here
     ```
5. **Run the app:**
   ```bash
   streamlit run app.py
   ```

---

## Usage
- Upload one or more PDF files in the sidebar.
- Click "Process Documents" to extract and embed text.
- Ask questions in the chat input; the chatbot will answer using the PDFs.
- Chat history and context are maintained for multi-turn conversations.
- Use "Clear Conversation" to reset memory.

---

## Architecture
- `app.py`: Streamlit UI and main app logic
- `src/processor.py`: PDF text extraction and chunking
- `src/embedding.py`: Embedding creation and FAISS vector store
- `src/chat.py`: Chat manager, LLM integration, conversational memory
- `src/config.py`: Centralized config, environment variable support
- `.env`: API keys and config
- `requirements.txt`: All dependencies
